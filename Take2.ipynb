{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TARGET = \"AI\"\n",
    "DATASET = \"dataset.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crunchbase_ID</th>\n",
       "      <th>home_text</th>\n",
       "      <th>aboutus_text</th>\n",
       "      <th>overview_text</th>\n",
       "      <th>whatwedo_text</th>\n",
       "      <th>company_text</th>\n",
       "      <th>whoweare_text</th>\n",
       "      <th>AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1916</td>\n",
       "      <td>Skip to main content Products GPU accelerated ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1917</td>\n",
       "      <td>Our AIs Research Company Careers Get in Touch ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our AIs Research Company Careers Get in Touch ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1918</td>\n",
       "      <td>Toggle navigation Product Projects Company His...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1919</td>\n",
       "      <td>Brainpeek Solutions Create a seamless online u...</td>\n",
       "      <td>Brainpeek Solutions Create a seamless online u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920</td>\n",
       "      <td>The Tool Our Languages Services Extract Produc...</td>\n",
       "      <td>The Tool Our Languages Services Extract Produc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>2735</td>\n",
       "      <td>Username or Email L senord Remember me Norsk S...</td>\n",
       "      <td>Username or Email L senord Remember me Norsk S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>5944</td>\n",
       "      <td>Solutions Solution for distributors Covered re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5251</td>\n",
       "      <td>BROWSE PRODUCTS Variety Cases Pasta Mac and Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>4225</td>\n",
       "      <td>Pricing Documentation Community Changelog Logi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>4779</td>\n",
       "      <td>FAQ Jobs Blog Contact FAQ Jobs Blog Contact Sc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4894 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     crunchbase_ID                                          home_text  \\\n",
       "0             1916  Skip to main content Products GPU accelerated ...   \n",
       "1             1917  Our AIs Research Company Careers Get in Touch ...   \n",
       "2             1918  Toggle navigation Product Projects Company His...   \n",
       "3             1919  Brainpeek Solutions Create a seamless online u...   \n",
       "4             1920  The Tool Our Languages Services Extract Produc...   \n",
       "...            ...                                                ...   \n",
       "4889          2735  Username or Email L senord Remember me Norsk S...   \n",
       "4890          5944  Solutions Solution for distributors Covered re...   \n",
       "4891          5251  BROWSE PRODUCTS Variety Cases Pasta Mac and Ch...   \n",
       "4892          4225  Pricing Documentation Community Changelog Logi...   \n",
       "4893          4779  FAQ Jobs Blog Contact FAQ Jobs Blog Contact Sc...   \n",
       "\n",
       "                                           aboutus_text overview_text  \\\n",
       "0                                                   NaN           NaN   \n",
       "1                                                   NaN           NaN   \n",
       "2                                                   NaN           NaN   \n",
       "3     Brainpeek Solutions Create a seamless online u...           NaN   \n",
       "4     The Tool Our Languages Services Extract Produc...           NaN   \n",
       "...                                                 ...           ...   \n",
       "4889  Username or Email L senord Remember me Norsk S...           NaN   \n",
       "4890                                                NaN           NaN   \n",
       "4891                                                NaN           NaN   \n",
       "4892                                                NaN           NaN   \n",
       "4893                                                NaN           NaN   \n",
       "\n",
       "     whatwedo_text                                       company_text  \\\n",
       "0              NaN                                                NaN   \n",
       "1              NaN  Our AIs Research Company Careers Get in Touch ...   \n",
       "2              NaN                                                NaN   \n",
       "3              NaN                                                NaN   \n",
       "4              NaN                                                NaN   \n",
       "...            ...                                                ...   \n",
       "4889           NaN                                                NaN   \n",
       "4890           NaN                                                NaN   \n",
       "4891           NaN                                                NaN   \n",
       "4892           NaN                                                NaN   \n",
       "4893           NaN                                                NaN   \n",
       "\n",
       "     whoweare_text  AI  \n",
       "0              NaN   1  \n",
       "1              NaN   1  \n",
       "2              NaN   1  \n",
       "3              NaN   1  \n",
       "4              NaN   1  \n",
       "...            ...  ..  \n",
       "4889           NaN   0  \n",
       "4890           NaN   0  \n",
       "4891           NaN   0  \n",
       "4892           NaN   0  \n",
       "4893           NaN   0  \n",
       "\n",
       "[4894 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(DATASET)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['home_text'].values.astype(str)\n",
    "y = df[TARGET].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokens = []\n",
    "tokens = [word_tokenize(str(sent)) for sent in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filterd_words = []\n",
    "for words in tokens:\n",
    "    x = [word for word in words if word not in stop_words] #if word not in stopwords]   \n",
    "    filterd_words.append(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# taking the root of every word\n",
    "lemmatized = []\n",
    "for words in filterd_words:\n",
    "    x = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    lemmatized.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking words that are bigger than 2    \n",
    "filterd = []\n",
    "for words in lemmatized:\n",
    "    x = [word for word in words if len(word) >= 2]\n",
    "    filterd.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(filterd, y, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 35\n",
    "\n",
    "def preprocess(X, tokenizer=None, padded=True):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>', lower=False)\n",
    "        tokenizer.fit_on_texts(X)\n",
    "        seq = tokenizer.texts_to_sequences(X)\n",
    "        tmp = seq\n",
    "        seq_padded = pad_sequences(seq, maxlen=input_length, padding='post', truncating='post')\n",
    "        return tokenizer, seq_padded\n",
    "    seq = tokenizer.texts_to_sequences(X)\n",
    "    seq_padded = pad_sequences(seq, maxlen=input_length, padding='post', truncating='post')\n",
    "    return seq_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, X_train_processed  = preprocess(X_train)\n",
    "X_val_processed = preprocess(X_val, tokenizer)\n",
    "\n",
    "input_dim = len(tokenizer.word_index)+1\n",
    "output_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "word_index = tokenizer.word_index\n",
    "with open('glove.6B.100d.txt','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], 'float32')\n",
    "        embedding_dict[word] = vectors\n",
    "\n",
    "max_words = input_dim\n",
    "embedding_dims = output_dim\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dims))\n",
    "for word, i in word_index.items():\n",
    "    if i > max_words:\n",
    "        continue\n",
    "        \n",
    "    emb_vec = embedding_dict.get(word)    \n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim, output_dim, input_length=input_length, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)),\n",
    "        tf.keras.layers.LSTM(20, activation='tanh', return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, verbose=0):\n",
    "    history = model.fit(X_train_processed, y_train, validation_data=(X_val_processed, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4404 samples, validate on 979 samples\n",
      "Epoch 1/20\n",
      "4404/4404 [==============================] - 9s 2ms/sample - loss: 0.6650 - accuracy: 0.5956 - val_loss: 0.6814 - val_accuracy: 0.5577\n",
      "Epoch 2/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.5089 - accuracy: 0.7548 - val_loss: 0.4897 - val_accuracy: 0.7753\n",
      "Epoch 3/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.3490 - accuracy: 0.8574 - val_loss: 0.4302 - val_accuracy: 0.8018\n",
      "Epoch 4/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.2374 - accuracy: 0.9171 - val_loss: 0.4150 - val_accuracy: 0.8151\n",
      "Epoch 5/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.1515 - accuracy: 0.9548 - val_loss: 0.4205 - val_accuracy: 0.8284\n",
      "Epoch 6/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.1004 - accuracy: 0.9737 - val_loss: 0.4716 - val_accuracy: 0.8294\n",
      "Epoch 7/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0687 - accuracy: 0.9839 - val_loss: 0.4861 - val_accuracy: 0.8386\n",
      "Epoch 8/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0526 - accuracy: 0.9884 - val_loss: 0.5598 - val_accuracy: 0.8304\n",
      "Epoch 9/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0418 - accuracy: 0.9916 - val_loss: 0.5689 - val_accuracy: 0.8274\n",
      "Epoch 10/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0322 - accuracy: 0.9939 - val_loss: 0.6005 - val_accuracy: 0.8335\n",
      "Epoch 11/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0270 - accuracy: 0.9952 - val_loss: 0.6586 - val_accuracy: 0.8366\n",
      "Epoch 12/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0196 - accuracy: 0.9968 - val_loss: 0.6843 - val_accuracy: 0.8417\n",
      "Epoch 13/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0198 - accuracy: 0.9952 - val_loss: 0.6877 - val_accuracy: 0.8355\n",
      "Epoch 14/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0405 - accuracy: 0.9902 - val_loss: 0.6663 - val_accuracy: 0.8304\n",
      "Epoch 15/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0237 - accuracy: 0.9939 - val_loss: 0.6918 - val_accuracy: 0.8315\n",
      "Epoch 16/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0231 - accuracy: 0.9946 - val_loss: 0.7093 - val_accuracy: 0.8325\n",
      "Epoch 17/20\n",
      "4404/4404 [==============================] - 7s 2ms/sample - loss: 0.0193 - accuracy: 0.9961 - val_loss: 0.6955 - val_accuracy: 0.8417\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "]\n",
    "model = make_model()\n",
    "summary = train_model(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979/979 [==============================] - 0s 100us/sample - loss: 0.6843 - accuracy: 0.8417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6843257069138694, 0.84167516]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val_processed, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
