{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "import re \n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# import tokenization\n",
    "from bert import tokenization\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"AI\"\n",
    "DATASET_PATH = \"dataset.pickle\"\n",
    "TRAIN_PATH = 'train.pkl'\n",
    "TEST_PATH = 'test.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crunchbase_ID</th>\n",
       "      <th>home_text</th>\n",
       "      <th>aboutus_text</th>\n",
       "      <th>overview_text</th>\n",
       "      <th>whatwedo_text</th>\n",
       "      <th>company_text</th>\n",
       "      <th>whoweare_text</th>\n",
       "      <th>AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1916</td>\n",
       "      <td>Skip to main content Products GPU accelerated ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1917</td>\n",
       "      <td>Our AIs Research Company Careers Get in Touch ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our AIs Research Company Careers Get in Touch ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1918</td>\n",
       "      <td>Toggle navigation Product Projects Company His...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1919</td>\n",
       "      <td>Brainpeek Solutions Create a seamless online u...</td>\n",
       "      <td>Brainpeek Solutions Create a seamless online u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920</td>\n",
       "      <td>The Tool Our Languages Services Extract Produc...</td>\n",
       "      <td>The Tool Our Languages Services Extract Produc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>2735</td>\n",
       "      <td>Username or Email L senord Remember me Norsk S...</td>\n",
       "      <td>Username or Email L senord Remember me Norsk S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>5944</td>\n",
       "      <td>Solutions Solution for distributors Covered re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5251</td>\n",
       "      <td>BROWSE PRODUCTS Variety Cases Pasta Mac and Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>4225</td>\n",
       "      <td>Pricing Documentation Community Changelog Logi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>4779</td>\n",
       "      <td>FAQ Jobs Blog Contact FAQ Jobs Blog Contact Sc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4894 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     crunchbase_ID                                          home_text  \\\n",
       "0             1916  Skip to main content Products GPU accelerated ...   \n",
       "1             1917  Our AIs Research Company Careers Get in Touch ...   \n",
       "2             1918  Toggle navigation Product Projects Company His...   \n",
       "3             1919  Brainpeek Solutions Create a seamless online u...   \n",
       "4             1920  The Tool Our Languages Services Extract Produc...   \n",
       "...            ...                                                ...   \n",
       "4889          2735  Username or Email L senord Remember me Norsk S...   \n",
       "4890          5944  Solutions Solution for distributors Covered re...   \n",
       "4891          5251  BROWSE PRODUCTS Variety Cases Pasta Mac and Ch...   \n",
       "4892          4225  Pricing Documentation Community Changelog Logi...   \n",
       "4893          4779  FAQ Jobs Blog Contact FAQ Jobs Blog Contact Sc...   \n",
       "\n",
       "                                           aboutus_text overview_text  \\\n",
       "0                                                   NaN           NaN   \n",
       "1                                                   NaN           NaN   \n",
       "2                                                   NaN           NaN   \n",
       "3     Brainpeek Solutions Create a seamless online u...           NaN   \n",
       "4     The Tool Our Languages Services Extract Produc...           NaN   \n",
       "...                                                 ...           ...   \n",
       "4889  Username or Email L senord Remember me Norsk S...           NaN   \n",
       "4890                                                NaN           NaN   \n",
       "4891                                                NaN           NaN   \n",
       "4892                                                NaN           NaN   \n",
       "4893                                                NaN           NaN   \n",
       "\n",
       "     whatwedo_text                                       company_text  \\\n",
       "0              NaN                                                NaN   \n",
       "1              NaN  Our AIs Research Company Careers Get in Touch ...   \n",
       "2              NaN                                                NaN   \n",
       "3              NaN                                                NaN   \n",
       "4              NaN                                                NaN   \n",
       "...            ...                                                ...   \n",
       "4889           NaN                                                NaN   \n",
       "4890           NaN                                                NaN   \n",
       "4891           NaN                                                NaN   \n",
       "4892           NaN                                                NaN   \n",
       "4893           NaN                                                NaN   \n",
       "\n",
       "     whoweare_text  AI  \n",
       "0              NaN   1  \n",
       "1              NaN   1  \n",
       "2              NaN   1  \n",
       "3              NaN   1  \n",
       "4              NaN   1  \n",
       "...            ...  ..  \n",
       "4889           NaN   0  \n",
       "4890           NaN   0  \n",
       "4891           NaN   0  \n",
       "4892           NaN   0  \n",
       "4893           NaN   0  \n",
       "\n",
       "[4894 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(DATASET_PATH)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0.0\n",
       "AI      0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "def build_model(bert_layer, lr, max_len=50):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    out = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabets(X):\n",
    "    regs = []\n",
    "    for sent in X:\n",
    "        x = re.sub('[^A-Za-z]',\" \",str(sent))\n",
    "        regs.append(x)\n",
    "    return regs\n",
    "\n",
    "\n",
    "def create_corpus(X):\n",
    "    \"\"\"Decompose text to corpus (e.g. `This is a pen` to [ `This`, `is`, `a`, `pen` ])\n",
    "    \n",
    "    Arguments:\n",
    "        texts: list(str) / Text list.\n",
    "        \n",
    "    Returns:\n",
    "        list(str) / Corpus list.\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    for sent in X:\n",
    "        words = [ word.lower() for word in word_tokenize(sent) ]\n",
    "        corpus.append(words)\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "\n",
    "def remove_stopwords(X):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filterd_words = []\n",
    "    keep = [\"n't\", \"not\", \"no\"]\n",
    "    for words in X:\n",
    "        x = [word for word in words if word not in stop_words or word in keep] \n",
    "        filterd_words.append(x)\n",
    "    return filterd_words\n",
    "\n",
    "\n",
    "def lemmatize(X):\n",
    "    \"\"\"taking the root of every word\"\"\"\n",
    "    lemmatized = []\n",
    "    for words in X:\n",
    "        x = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "        lemmatized.append(x)\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def remove_unnecessary_words(X):\n",
    "    \"\"\"taking words that are bigger than 2\"\"\"\n",
    "    filterd = []\n",
    "    for words in X:\n",
    "        x = [word for word in words if len(word) >= 2]\n",
    "        filterd.append(x)\n",
    "    return filterd    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home_text = df[['home_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_aboutus_text = df[df['aboutus_text'].notnull()][['aboutus_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_overview_text = df[df['overview_text'].notnull()][['overview_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_whatwedo_text = df[df['whatwedo_text'].notnull()][['whatwedo_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_company_text = df[df['company_text'].notnull()][['company_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_whoweare_text = df[df['whoweare_text'].notnull()][['whoweare_text', 'AI']].set_axis( ['text', 'AI'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_home_text, df_aboutus_text, df_overview_text, df_whatwedo_text, df_company_text, df_whoweare_text], axis=0, ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].values.astype(str)\n",
    "y = df[TARGET].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X = remove_non_alphabets(X)\n",
    "X = create_corpus(X)\n",
    "X = remove_stopwords(X)\n",
    "X = lemmatize(X)\n",
    "X = remove_unnecessary_words(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [\" \".join(x) for x in X]\n",
    "X = np.array(tmp, dtype='object')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [X_train, y_train]\n",
    "test = [X_test, y_test]\n",
    "joblib.dump(train, 'train.pkl')\n",
    "joblib.dump(test, \"test.pkl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set = joblib.load(TRAIN_PATH)\n",
    "# X_train = train_set[0]\n",
    "# y_train = train_set[1]\n",
    "\n",
    "# test_set = joblib.load(TEST_PATH)\n",
    "# X_test = test_set[0]\n",
    "# y_test = test_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"bert_model\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 35\n",
    "X_train = bert_encode(X_train, tokenizer, max_len=max_len)\n",
    "X_test = bert_encode(X_test, tokenizer, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 35)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "model = build_model(bert_layer, lr, max_len=max_len)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6302 samples, validate on 701 samples\n",
      "Epoch 1/10\n",
      "6302/6302 [==============================] - 431s 68ms/sample - loss: 0.4851 - accuracy: 0.7598 - val_loss: 0.3728 - val_accuracy: 0.8445\n",
      "Epoch 2/10\n",
      "6302/6302 [==============================] - 398s 63ms/sample - loss: 0.2266 - accuracy: 0.9080 - val_loss: 0.3785 - val_accuracy: 0.8531\n",
      "Epoch 3/10\n",
      "6302/6302 [==============================] - 390s 62ms/sample - loss: 0.0654 - accuracy: 0.9797 - val_loss: 0.4102 - val_accuracy: 0.8716\n",
      "Epoch 4/10\n",
      "6302/6302 [==============================] - 391s 62ms/sample - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.4735 - val_accuracy: 0.8759\n",
      "Epoch 5/10\n",
      "6302/6302 [==============================] - 393s 62ms/sample - loss: 0.0272 - accuracy: 0.9916 - val_loss: 0.5194 - val_accuracy: 0.8787\n",
      "Epoch 6/10\n",
      "6302/6302 [==============================] - 340s 54ms/sample - loss: 0.0258 - accuracy: 0.9900 - val_loss: 0.6672 - val_accuracy: 0.8559\n",
      "Epoch 7/10\n",
      "6302/6302 [==============================] - 341s 54ms/sample - loss: 0.0284 - accuracy: 0.9914 - val_loss: 0.6534 - val_accuracy: 0.8559\n",
      "Epoch 8/10\n",
      "6302/6302 [==============================] - 342s 54ms/sample - loss: 0.0063 - accuracy: 0.9975 - val_loss: 0.6398 - val_accuracy: 0.8688\n",
      "Wall time: 50min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "checkpoint = ModelCheckpoint('bert_model_2.h5', monitor='val_accuracy', save_best_only=True)\n",
    "early_stoping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint, early_stoping],\n",
    "    batch_size=6   # so low because of the memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bert_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 9s 11ms/sample - loss: 0.6599 - accuracy: 0.8460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6599028228336182, 0.8459563]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']         # train loss\n",
    "train_acc = history.history['accuracy']      # train accuracy\n",
    "val_loss = history.history['val_loss']       # validation loss\n",
    "val_acc = history.history['val_accuracy']    # validation accuracy\n",
    "\n",
    "# Plotting \n",
    "plt.figure(figsize=(20, 8))   # figure size\n",
    "\n",
    "plt.subplot(1, 2, 1)          # first plot: loss plot\n",
    "# line plot\n",
    "plt.plot( train_loss, label='train loss')   # train loss line plot\n",
    "plt.plot( val_loss, label='val loss')       # validation loss line plot\n",
    "\n",
    "plt.title('Loss')     # plot title\n",
    "plt.legend()          # to display labels\n",
    "\n",
    "plt.subplot(1, 2, 2)         # second plot: accuracy plot\n",
    "# line plot\n",
    "plt.plot(train_acc, label='train accuracy')    # train accuracy line plot\n",
    "plt.plot(val_acc, label='val accuracy')        # validation accuracy line plot\n",
    "\n",
    "plt.title('Accuracy')    # plot title\n",
    "plt.legend()             # to display labels\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test_processed).round().astype(int)\n",
    "\n",
    "conf = confusion_matrix(y_test, preds)\n",
    "sns.heatmap(conf, annot=True);\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
