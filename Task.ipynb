{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TARGET = \"AI\"\n",
    "DATASET = \"dataset.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crunchbase_ID</th>\n",
       "      <th>home_text</th>\n",
       "      <th>aboutus_text</th>\n",
       "      <th>overview_text</th>\n",
       "      <th>whatwedo_text</th>\n",
       "      <th>company_text</th>\n",
       "      <th>whoweare_text</th>\n",
       "      <th>AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1916</td>\n",
       "      <td>Skip to main content Products GPU accelerated ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1917</td>\n",
       "      <td>Our AIs Research Company Careers Get in Touch ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our AIs Research Company Careers Get in Touch ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1918</td>\n",
       "      <td>Toggle navigation Product Projects Company His...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1919</td>\n",
       "      <td>Brainpeek Solutions Create a seamless online u...</td>\n",
       "      <td>Brainpeek Solutions Create a seamless online u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920</td>\n",
       "      <td>The Tool Our Languages Services Extract Produc...</td>\n",
       "      <td>The Tool Our Languages Services Extract Produc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>2735</td>\n",
       "      <td>Username or Email L senord Remember me Norsk S...</td>\n",
       "      <td>Username or Email L senord Remember me Norsk S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>5944</td>\n",
       "      <td>Solutions Solution for distributors Covered re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>5251</td>\n",
       "      <td>BROWSE PRODUCTS Variety Cases Pasta Mac and Ch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>4225</td>\n",
       "      <td>Pricing Documentation Community Changelog Logi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>4779</td>\n",
       "      <td>FAQ Jobs Blog Contact FAQ Jobs Blog Contact Sc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4894 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     crunchbase_ID                                          home_text  \\\n",
       "0             1916  Skip to main content Products GPU accelerated ...   \n",
       "1             1917  Our AIs Research Company Careers Get in Touch ...   \n",
       "2             1918  Toggle navigation Product Projects Company His...   \n",
       "3             1919  Brainpeek Solutions Create a seamless online u...   \n",
       "4             1920  The Tool Our Languages Services Extract Produc...   \n",
       "...            ...                                                ...   \n",
       "4889          2735  Username or Email L senord Remember me Norsk S...   \n",
       "4890          5944  Solutions Solution for distributors Covered re...   \n",
       "4891          5251  BROWSE PRODUCTS Variety Cases Pasta Mac and Ch...   \n",
       "4892          4225  Pricing Documentation Community Changelog Logi...   \n",
       "4893          4779  FAQ Jobs Blog Contact FAQ Jobs Blog Contact Sc...   \n",
       "\n",
       "                                           aboutus_text overview_text  \\\n",
       "0                                                   NaN           NaN   \n",
       "1                                                   NaN           NaN   \n",
       "2                                                   NaN           NaN   \n",
       "3     Brainpeek Solutions Create a seamless online u...           NaN   \n",
       "4     The Tool Our Languages Services Extract Produc...           NaN   \n",
       "...                                                 ...           ...   \n",
       "4889  Username or Email L senord Remember me Norsk S...           NaN   \n",
       "4890                                                NaN           NaN   \n",
       "4891                                                NaN           NaN   \n",
       "4892                                                NaN           NaN   \n",
       "4893                                                NaN           NaN   \n",
       "\n",
       "     whatwedo_text                                       company_text  \\\n",
       "0              NaN                                                NaN   \n",
       "1              NaN  Our AIs Research Company Careers Get in Touch ...   \n",
       "2              NaN                                                NaN   \n",
       "3              NaN                                                NaN   \n",
       "4              NaN                                                NaN   \n",
       "...            ...                                                ...   \n",
       "4889           NaN                                                NaN   \n",
       "4890           NaN                                                NaN   \n",
       "4891           NaN                                                NaN   \n",
       "4892           NaN                                                NaN   \n",
       "4893           NaN                                                NaN   \n",
       "\n",
       "     whoweare_text  AI  \n",
       "0              NaN   1  \n",
       "1              NaN   1  \n",
       "2              NaN   1  \n",
       "3              NaN   1  \n",
       "4              NaN   1  \n",
       "...            ...  ..  \n",
       "4889           NaN   0  \n",
       "4890           NaN   0  \n",
       "4891           NaN   0  \n",
       "4892           NaN   0  \n",
       "4893           NaN   0  \n",
       "\n",
       "[4894 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(DATASET)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4894 entries, 0 to 4893\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   crunchbase_ID  4894 non-null   object\n",
      " 1   home_text      4894 non-null   object\n",
      " 2   aboutus_text   2212 non-null   object\n",
      " 3   overview_text  66 non-null     object\n",
      " 4   whatwedo_text  50 non-null     object\n",
      " 5   company_text   477 non-null    object\n",
      " 6   whoweare_text  83 non-null     object\n",
      " 7   AI             4894 non-null   int64 \n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 306.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crunchbase_ID    0.000000\n",
       "home_text        0.000000\n",
       "aboutus_text     0.548018\n",
       "overview_text    0.986514\n",
       "whatwedo_text    0.989783\n",
       "company_text     0.902534\n",
       "whoweare_text    0.983040\n",
       "AI               0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_home_text = df[['home_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_aboutus_text = df[df['aboutus_text'].notnull()][['aboutus_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_overview_text = df[df['overview_text'].notnull()][['overview_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_whatwedo_text = df[df['whatwedo_text'].notnull()][['whatwedo_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_company_text = df[df['company_text'].notnull()][['company_text', 'AI']].set_axis( ['text', 'AI'], axis=1)\n",
    "df_whoweare_text = df[df['whoweare_text'].notnull()][['whoweare_text', 'AI']].set_axis( ['text', 'AI'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_home_text, df_aboutus_text, df_overview_text, df_whatwedo_text, df_company_text, df_whoweare_text], axis=0, ignore_index=True)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7782 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      AI\n",
       "7678   0\n",
       "1926   1\n",
       "2280   1\n",
       "2333   1\n",
       "101    1\n",
       "...   ..\n",
       "5226   1\n",
       "5390   1\n",
       "860    1\n",
       "7603   0\n",
       "7270   1\n",
       "\n",
       "[7782 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_vars = [var for var in df.columns if df[var].isnull().sum() > 0]\n",
    "df[null_vars+[\"AI\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].values.astype(str)\n",
    "y = df[TARGET].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def create_corpus(texts):\n",
    "    \"\"\"Decompose text to corpus (e.g. `This is a pen` to [ `This`, `is`, `a`, `pen` ])\n",
    "    \n",
    "    Arguments:\n",
    "        texts: list(str) / Text list.\n",
    "        \n",
    "    Returns:\n",
    "        list(str) / Corpus list.\n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = []\n",
    "    for tweet in texts:\n",
    "        words = [ word.lower() for word in word_tokenize(tweet) ]\n",
    "        corpus.append(words)\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "tokens = create_corpus(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filterd_words = []\n",
    "for words in tokens:\n",
    "    x = [word for word in words if word not in stop_words] #if word not in stopwords]   \n",
    "    filterd_words.append(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the root of every word\n",
    "lemmatized = []\n",
    "for words in filterd_words:\n",
    "    x = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "    lemmatized.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking words that are bigger than 2    \n",
    "filterd = []\n",
    "for words in lemmatized:\n",
    "    x = [word for word in words if len(word) >= 2]\n",
    "    filterd.append(x)\n",
    "    \n",
    "X = filterd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 35\n",
    "\n",
    "def preprocess(X, tokenizer=None, padded=True):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "        tokenizer.fit_on_texts(X)\n",
    "        seq = tokenizer.texts_to_sequences(X)\n",
    "        tmp = seq\n",
    "        seq_padded = pad_sequences(seq, maxlen=input_length, padding='post', truncating='post')\n",
    "        return tokenizer, seq_padded\n",
    "    seq = tokenizer.texts_to_sequences(X)\n",
    "    seq_padded = pad_sequences(seq, maxlen=input_length, padding='post', truncating='post')\n",
    "    return seq_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, X_train_processed  = preprocess(X_train)\n",
    "X_val_processed = preprocess(X_val, tokenizer)\n",
    "\n",
    "input_dim = len(tokenizer.word_index)+1\n",
    "output_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = {}\n",
    "word_index = tokenizer.word_index\n",
    "with open('glove.6B.100d.txt','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], 'float32')\n",
    "        embedding_dict[word] = vectors\n",
    "\n",
    "max_words = input_dim\n",
    "embedding_dims = output_dim\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dims))\n",
    "for word, i in word_index.items():\n",
    "    if i > max_words:\n",
    "        continue\n",
    "        \n",
    "    emb_vec = embedding_dict.get(word)    \n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i] = emb_vec  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim, output_dim, input_length=input_length, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix)),\n",
    "        tf.keras.layers.LSTM(20, activation='tanh', return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, verbose=0):\n",
    "    history = model.fit(X_train_processed, y_train, validation_data=(X_val_processed, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, verbose=verbose)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_training(times=30):\n",
    "    scores = pd.DataFrame(columns=['N', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n",
    "    for i in range(1, times+1):\n",
    "        model = make_model()\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                           optimizer=\"adam\",\n",
    "                           metrics=[\"accuracy\"])\n",
    "    \n",
    "        history = train_model(model)\n",
    "        \n",
    "        train_loss, train_acc = model.evaluate(X_train_processed, y_train, verbose=0)\n",
    "        val_loss, val_acc = model.evaluate(X_val_processed, y_val, verbose=0)\n",
    "        \n",
    "        scores.loc[i, 'N'] = i\n",
    "        scores.loc[i, 'train_loss'] = train_loss\n",
    "        scores.loc[i, 'train_acc'] = train_acc\n",
    "        scores.loc[i, 'val_loss'] = val_loss\n",
    "        scores.loc[i, 'val_acc'] = val_acc\n",
    "        \n",
    "    scores['train_loss'] = scores['train_loss'].astype(float)\n",
    "    scores['train_acc'] = scores['train_acc'].astype(float)\n",
    "    scores['val_loss'] = scores['val_loss'].astype(float)\n",
    "    scores['val_acc'] = scores['val_acc'].astype(float)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6225 samples, validate on 1557 samples\n",
      "Epoch 1/20\n",
      "6225/6225 [==============================] - 12s 2ms/sample - loss: 0.6498 - accuracy: 0.6104 - val_loss: 0.5560 - val_accuracy: 0.7238\n",
      "Epoch 2/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.4907 - accuracy: 0.7719 - val_loss: 0.4985 - val_accuracy: 0.7437\n",
      "Epoch 3/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.3732 - accuracy: 0.8437 - val_loss: 0.4510 - val_accuracy: 0.7964\n",
      "Epoch 4/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.2843 - accuracy: 0.8863 - val_loss: 0.4539 - val_accuracy: 0.7951\n",
      "Epoch 5/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.2055 - accuracy: 0.9301 - val_loss: 0.5271 - val_accuracy: 0.8041\n",
      "Epoch 6/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.1459 - accuracy: 0.9516 - val_loss: 0.5395 - val_accuracy: 0.8118\n",
      "Epoch 7/20\n",
      "6225/6225 [==============================] - 9s 2ms/sample - loss: 0.1064 - accuracy: 0.9672 - val_loss: 0.5435 - val_accuracy: 0.8131\n",
      "Epoch 8/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0822 - accuracy: 0.9759 - val_loss: 0.5960 - val_accuracy: 0.8163\n",
      "Epoch 9/20\n",
      "6225/6225 [==============================] - 9s 2ms/sample - loss: 0.0692 - accuracy: 0.9807 - val_loss: 0.6592 - val_accuracy: 0.8215\n",
      "Epoch 10/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0481 - accuracy: 0.9875 - val_loss: 0.7310 - val_accuracy: 0.8176\n",
      "Epoch 11/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0445 - accuracy: 0.9884 - val_loss: 0.7111 - val_accuracy: 0.8240\n",
      "Epoch 12/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0346 - accuracy: 0.9916 - val_loss: 0.6946 - val_accuracy: 0.8137\n",
      "Epoch 13/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0295 - accuracy: 0.9929 - val_loss: 0.8057 - val_accuracy: 0.8150\n",
      "Epoch 14/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0334 - accuracy: 0.9908 - val_loss: 0.8198 - val_accuracy: 0.8118\n",
      "Epoch 15/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0275 - accuracy: 0.9920 - val_loss: 0.8325 - val_accuracy: 0.8144\n",
      "Epoch 16/20\n",
      "6225/6225 [==============================] - 9s 1ms/sample - loss: 0.0232 - accuracy: 0.9936 - val_loss: 0.8513 - val_accuracy: 0.8170\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "]\n",
    "model = make_model()\n",
    "summary = train_model(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557/1557 [==============================] - 0s 95us/sample - loss: 0.7111 - accuracy: 0.8240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7111025462567079, 0.82402056]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val_processed, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
